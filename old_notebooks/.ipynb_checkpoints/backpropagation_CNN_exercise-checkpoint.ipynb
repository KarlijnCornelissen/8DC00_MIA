{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding and implementing a neural netwok\n",
    "\n",
    "This erxercise consists of two sub exercises, that will help you to understand how backpropagation is carried out and how a neural network is implemented.\n",
    "\n",
    "A) Backpropagation - Simplest back propagation explanation and exercise (with pen and paper)\n",
    "\n",
    "B) Implementation of a convolutional neural network (with python)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Backpropataion\n",
    "\n",
    "\n",
    "### Simple neural network\n",
    "For this exercise we will walk through the absolute simplest backpropagation example. The neural network that will be used, is as follows:\n",
    "\n",
    "<img src=\"assets/01_simple_network.jpg\\\" width=\"400\\\" align=\"center\\\"/>\n",
    "\n",
    "This network is used to predict a value of $\\hat{y}$, given the input $x$, where both $x$ and $y$ are scalars. This network contains only one fully connected layer (without a bias), therefore the output can be calculated as $\\hat{y}=w\\cdot x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set\n",
    "The model needs to be trained to obtain the optimal value for the weight $w$. Normally a large training set is used to find the optimal values for all the weights, but for simplification purposes, our training set consists of a single input-output pair, which is as follows:\n",
    "\n",
    "\n",
    "| Input ($x$) | Desired output ($y$) |\n",
    "| :--- | :--- |\n",
    "| 1.5 | 0.5 |\n",
    "\n",
    "Because this is such an easy example we know that the solution to this optimization problem is $w = \\frac{y}{x} = \\frac{0.5}{1.5} \\approx 0.33$. However, normally neural networks are used for much more complex optimization problems with millions of parameters (weights) and many more training examples. Therefore, the best solution cannot just simply be calculated like this, and an iterative training approach is needed where the network weights are optimized one step at a time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model initialization\n",
    "This optimization process predicts the output $\\hat{y}$ given an input $x$ and a weight $w$. Subsequently the weight $w$ is updated such that the predicted output $\\hat{y}$ becomes more similar to $y$. To start this optimization process, the model weight $w$ is initialized with a random value, let's say $0.8$. We can now calculate the current value (after zero epochs, i.e. at initialization) of $\\hat{y}$, given that $x=1.5$ and $w=0.8$:\n",
    "\n",
    "|Epoch | Input ($x$) | Desired output ($y$) | Weight ($w$) | Predicted output ($\\hat{y}$) |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "|0 (init) | 1.5 | 0.5 | 0.8 | 1.2 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & loss function\n",
    "Now the question arises of how the model needs to be trained such that the current output reaches the desired output of $0.5$. For this training process, a loss function is defined. The model will try to minimize the value of the loss function, and therefore the loss function gives the model feedback on how the network weights should be updated. The loss function of this example is defined as the squared difference between the predicted and desired output:\n",
    "\n",
    "\\begin{equation}\n",
    "L = (\\hat{y} - y)^2\n",
    "\\end{equation}\n",
    "\n",
    "The loss function is visualized for the given training pair in the following figure. It can be seen that the loss function is parabola with a minimum around $0.33$ (green dot), which is in line with the solution we calculated earlier. \n",
    "\n",
    "<img src=\"assets/02_Loss_function.jpg\\\" width=\"500\\\" align=\"center\\\"/>\n",
    "\n",
    "The backpropagation algorithm seeks to minimize the loss by descending along the loss function (red arrow), which is called gradient descent. To take a descending step in the direction of the slope, the derivative of the loss function needs to be calculated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCISE A1\n",
    "* Given the model $\\hat{y}=wx$ and the loss function $L = (\\hat{y} - y)^2$, find the derivative of the loss function with respect to the weight: $\\frac{\\partial L}{\\partial w}$. (Tip: Use the chain rule: $\\frac{\\partial L}{\\partial w}=\\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial w}$)\n",
    "\n",
    "* Fill in the values of the training set: $x=1.5$ and $y=0.5$.\n",
    "\n",
    "\n",
    "**ANSWER** \n",
    "\n",
    "* $\\frac{\\partial L}{\\partial w} = 2x(wx-y)$\n",
    "\n",
    "* $\\frac{\\partial L}{\\partial w}(x=1.5, y=0.5) = 4.5w-1.5$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate\n",
    "After calculating the derivative, the model weight is updated by taking a step along the slope. Therefore, a step size needs to be formulated with care. If the step size is too small, it will take many steps before the minimum is reaches. But if the step size is too large, the minimum will not exactly be reached because the red dot 'bounces' around the minimum. This step size is usually called the learning rate. For this example, we take a learning rate ($r$) of $0.1$.\n",
    "\n",
    "Now weight can be updated according to the gradient descent and the learning rate as follows:\n",
    "\\begin{equation}\n",
    "w_{new} = w_{old} - r \\frac{\\partial L}{\\partial w}\n",
    "\\end{equation}\n",
    "\n",
    "After one step (i.e. after one epoch, since we have a training set size of 1), the weight is updated to \n",
    "\n",
    "\\begin{equation}\n",
    "w_{new} = 0.8 - 0.1 \\frac{\\partial L}{\\partial w}(x=1.5, y=0.5, w=0.8) \\approx 0.59,\n",
    "\\end{equation}\n",
    "\n",
    "Which means that the updated predicted value is: $\\hat{y}=0.59\\cdot 1.5 \\approx 0.89$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCISE A2\n",
    "Calculate the weights and predicted outputs for the next epochs until the model converges (i.e. the weight is approximately 0.33). Fill in (and continue) the following table:\n",
    "\n",
    "|Epoch | Input ($x$) | Desired output ($y$) | Weight ($w$) | Predicted output ($\\hat{y}$) |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "|0 (init) | 1.5 | 0.5 | 0.8 | 1.2 |\n",
    "|1 | 1.5 | 0.5 | 0.59 | 0.89 |\n",
    "|2 | 1.5 | 0.5 |  |  |\n",
    "|3 | 1.5 | 0.5 |  |  |\n",
    "|.. | .. | .. |  |  |\n",
    "|n | 1.5 | 0.5 |  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION A1\n",
    "After how many epochs does the model converge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following model training\n",
    "Normally this process is done using python because the model is much more complex.But you still want to follow the training process to know when the model is done training. For this you can plot the loss values while training. To mimic this, we are also going to plot the loss curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCISE A3\n",
    "Complete the list w (line 5, of the following python script) with the values of the weights that you calculated in the previous exercise. When the list is complete, run the cell and inspect the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'ellipsis' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d3e4bb07134b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'ellipsis' and 'float'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Fill in the values of the weights of all the epochs (4th column in the previous table)\n",
    "w = [0.8, 0.59, ...]\n",
    "# ANSWER: w = [0.8, 0.59, 0.4745, 0.4109, 0.37603, 0.356]\n",
    "\n",
    "# Define inputs\n",
    "x = 1.5\n",
    "y = 0.5\n",
    "\n",
    "# M\n",
    "weights = np.array(w)\n",
    "epochs = np.arange(len(w))\n",
    "L = (weights*x - y)**2\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, L)\n",
    "plt.title('Loss value over time')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION A2\n",
    "What do you think of the training process when looking at the loss plot resulting from exercise 3? \n",
    "* Do you think the model trained long enough? Explain your answer. \n",
    "* Do you think the step size of 0.1 was appropriate for the given model? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Convolutional neural network implementation\n",
    "\n",
    "### Hypothetical population\n",
    "For the second part of this practical session we will implement a simple neural network using pytorch. The goal of this model is to distinguish healthy people from sick people. Imagine this hypothetical population where all people with a straight head are healthy and all people with a head that is rotated 90 degrees to the left or the right are 'sick'. An example of brain MRI data from this population is given in the following figure:\n",
    "\n",
    "<img src=\"assets/03_Brain_downsampled.jpg\\\" width=\"1000\\\" align=\"center\\\"/>\n",
    "\n",
    "As mentioned before, this will be a simple implementation, therefore all high-resolution images are downsampled to low-resolution 'MRI' images of 3x3. These low-resolution images are shown in the second row. The task for this exercise is to implement a neural network that can be trained on the 3x3 images to distinguish sick ('horizontal lines') from healthy ('vertical lines') subjects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "To train and test the model, a toy training and test dataset can be loaded by running the following python cell. \n",
    "* The training set consists of 100 3x3 'brain MRI' images with the corresponding label of healthy (0) or sick (1). These labels can be seen as the desired output ($y$) in part A of this practical session.\n",
    "* The test set consists of 10 3x3 'brain MRI' images with the corresponding labels, which are used to compare the model performance to the ground truth (the labels).\n",
    "\n",
    "#### Exercise B1\n",
    "Run the following python cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD datasets as large numpy arrays\n",
    "# train_set = np array [100, 3, 3]\n",
    "# test_set = np array [10, 3, 3]\n",
    "\n",
    "#.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
